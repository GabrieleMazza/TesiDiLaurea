\documentclass[a4paper,11pt,twoside,openright]{book}							% COMANDI INIZIALI


\usepackage[italian]{babel}								% sillabazione italiana
\usepackage[utf8]{inputenc}								% Per le lettere accentate IN UNIX E IN WINDOWS

\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{float}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norma}{\lVert}{\rVert}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\chapter{Descrizione del modello}

\section{Caso senza covariate}

\subsection*{Dati e modello}

Siano $\{\underline p_i = (x_i,y_i); i=1, ... , n\}$ un insieme di $n$ punti spaziali in un dominio limitato $\Omega \subset \mathbb R^2$ e siano $\{t_j ; j=1, ... , m\}$ un insieme di $m$ istanti temporali in un intervallo $[0,T]\subset \mathbb R$. In questi punti ed istanti osserviamo i dati: siano quindi $z_{ij}$ i valori della variabile reale nel punto $\underline p_i$ al tempo $t_j$.

Supponiamo che le osservazioni $\{ z_{ij};i=1, ... , n; j=1, ... , m \}$ provengano da una funzione $f(\underline p, t)$, con l'aggiunta di un rumore:
\begin{equation}
\label{eq:modellobase}
z_{ij}=f(\underline p_i,t_j)+\epsilon_{ij}\ \ \ \ i = 1,...,n\ \ j=1,...,m \ \ ,
\end{equation}
dove $\{\epsilon_{ij}; i = 1,...,n; j=1,...m\}$ sono residui indipendenti identicamente distribuiti di media nulla e varianza $\sigma^2$.

L'obiettivo del modello è stimare la funzione spazio-temporale $f(\underline p, t)$ dai dati, minimizzando il seguente funzionale:
\begin{multline}
\label{eq:Jfunc}
J_{\underline \lambda }(f) = \sum_{i=1}^n \sum_{j=1}^m \bigl( z_{ij} - f(\underline p_i,t_j) \bigr)^2 \ + \\+\   \lambda_S \int_0^T \int_\Omega \Bigl( \triangle f  \Bigr)^2 d\Omega\ dt \ +\  \lambda_T \int_\Omega \int_0^T \Bigl( \frac{\partial^2 f }{\partial t ^2} \Bigr)^2 dt\ d\Omega .
\end{multline}
Il primo termine di $J_{\underline \lambda }(f)$ considera la minimizzazione dello scarto quadratico tra i dati e la funzione $f$ calcolata nei corrispondenti punti spaziali e istanti temporali. Tuttavia in aggiunta il funzionale presenta due termini di penalizzazione, che nel processo di minimizzazione cercano di rendere liscia e regolare la funzione rispettivamente in spazio e tempo.

Il termine della penalizzazione in spazio comprende l'integrale sull'intervallo $[0,T]$ dell'integrale sul dominio spaziale $\Omega$ del quadrato del laplaciano della funzione $f$. Come è noto, il laplaciano esprime la curvatura della funzione, ed è quindi una misura di quanto la funzione è liscia in spazio. L'analogo significato in tempo è rappresentato dalla derivata seconda in $t$, che nell'ultimo termine della penalizzazione è integrata prima sull'intervallo temporale $[0,T]$, e in seguito sul dominio spaziale $\Omega$.

I due termini $\lambda_S$ e $\lambda_T$ sono rispettivamente i pesi della penalizzazione in spazio e in tempo. La scelta di $\underline \lambda$, vettore $ (\lambda_S,\lambda_T) $ deve essere molto accurata. Infatti, valori troppo bassi per i due termini causerebbero una stima vicina all'interpolazione dei dati (poiché darebbero più peso al termine con gli scarti quadratici), mentre valori troppo elevati porterebbero ad avere una funzione $f$ fin troppo liscia e quindi distante dai dati. Per questo motivo sarà dato ampio spazio alla scelta di $\underline \lambda$.

\subsection*{Spazio funzionale per $f$}

\subsection*{Sviluppo in funzioni di base della funzione $f$}

Per poter risolvere il problema numericamente è necessaria una riduzione finito-dimensionale della funzione $f$. Rappresentiamo quindi $f$ con un opportuno sviluppo di basi separate in spazio e tempo.

Sia $\{\varphi_i(\underline p); i=1, ... , N\}$ un insieme di $N$ basi spaziali definite sul dominio $\Omega$ e $\{\psi_j(t); j=1, ... , M\}$ un insieme di basi temporali definite sull'intervallo $[0,T]$. Da questi due insiemi di basi marginali in spazio e tempo si costruisce la funzione $f$, con tutti i prodotti incrociati disponibili tra spazio e tempo:
\begin{equation} 
\label{eq:basisexp}
f(\underline p,t)=\sum_{i=1}^n \sum_{j=1}^m c_{ij}\ \varphi_i(\underline p)\ \psi_j(t) .
\end{equation}
Nel codice implementato sono disponibili elementi finiti in spazio e splines in tempo, perciò il dominio $\Omega$ sarà sostituito dalla sua triangolazione $\Omega_T$.

La funzione $f(\underline p,t)$ può essere identificata semplicemente con i valori dei coefficienti $\{c_{ij}; i=1, ... , N\ j=1, ... , M\}$. Quindi l'obiettivo della stima sarà il vettore contenente questi coefficienti:
\begin{equation}
\underline c =
\begin{bmatrix}
c_{11}  \\
\vdots\\
c_{1M}  \\
c_{21}  \\
\vdots\\
c_{2M}  \\
\vdots\\
c_{NM}
\end{bmatrix}.
\end{equation}
e si dimostrerà che sarà soluzione di un sistema lineare.

\subsection*{Discretizzazione dei termini di penalizzazione di $J$}
Dopo aver fissato le basi dello sviluppo di $f$, è necessario riscrivere in forma discreta anche il funzionale $J$ in (\ref{eq:Jfunc}). La parte più complessa da trattare di $J$ è rappresentata dai due termini di penalizzazione in spazio e tempo, che si semplifica considerando la seguente discretizzazione:
\begin{equation}
\label{eq:penalizzdisc}
\lambda_S  \sum_{j=1}^M \int_{\Omega_T} \Bigl( \triangle(\sum_{i=1}^N  c_{ij}\varphi_i ) \Bigr)^2 d \Omega + \lambda_T \sum_{i=1}^N\int_0^T \Bigl( \frac{\partial^2 \sum_{j=1}^M(c_{ij}\psi_j) }{\partial t ^2} \Bigr)^2 dt ,
\end{equation}
La scelta di questa discretizzazione è giustificata dalla separabilità delle due penalizzazioni in $J$ e dalla forma di $f$ nell'espressione in funzioni di base in \ref{eq:basisexp}. Infatti $f$ è ricavata combinando le basi dei modelli marginali in spazio e in tempo, e si può ricavare che:
$$
f(\underline p,t)=\sum_{j=1}^Mm_{S_j}(\underline p)\psi(t)=\sum_{i=0}^N\varphi_i(\underline p)m_{T_i}(t)
$$
dove
$$
m_{S_j}(\underline p)=\sum_{i=0}^Nc_{ij}\varphi_i(\underline p) \qquad \forall j=1...M
$$
$$
m_{T_i}(t)=\sum_{j=0}^Mc{ij}\psi_j(t) \qquad \forall i=1...N
$$
cioè da $f$ si possono ricavare due insiemi di funzioni marginali fissando rispettivamente l'indice in tempo o in spazio.

Inoltre lo smoothing marginale spaziale ha il suo termine di penalizzazione (l'integrale su $\Omega_T$ del laplaciano, che indicheremo con $J_S$) così come lo smoothing in tempo (l'integrale su $[0,T]$ della derivata seconda, che indicheremo con $J_T$). Tali modelli marginali sono già stati trattati con le stesse basi dei casi che saranno analizzati in seguito (elementi finiti in spazio e B-spline in tempo) e sono validi, quindi per discretizzare $J$ secondo quanto indicato in \ref{eq:penalizzdisc} basta applicare $J_S$ e $J_T$ alle restrizioni marginali risultanti dallo sviluppo di $f$:
\begin{multline*}
\lambda_S  \sum_{j=1}^M J_S(m_{S_j}(\underline p)) +
\lambda_T \sum_{i=1}^N J_T(m_{T_i}(t)) =\\
=\lambda_S  \sum_{j=1}^M \int_{\Omega_T} \Bigl( \triangle(\sum_{i=1}^N  c_{ij}\varphi_i ) \Bigr)^2 d \Omega + \lambda_T \sum_{i=1}^N\int_0^T \Bigl( \frac{\partial^2 \sum_{j=1}^M(c_{ij}\psi_j) }{\partial t ^2} \Bigr)^2 dt
\end{multline*}
Dopo questa costruzione (analoga a quella riportata in \cite{art:marra}) non resta che calcolare i due termini con gli integrali in \ref{eq:penalizzdisc}.

L'integrale $\int_{\Omega_T} ( \triangle(\sum_{i=1}^N  c_{ij}\varphi_i ) )^2 d \Omega$ implica la creazione della matrice $S_S$, il cui (k,l)-mo elemento è:
$$S_{S_{k,l}} = \int_\Omega \triangle \varphi_k \triangle \varphi_l \ \ \ k = 1, ... , n\ \ l = 1, ... , n.$$
Per avere una forma utile a livello computazionale di questa espressione è necessario considerare $ g = \triangle\varphi_l $ e un insieme di funzioni test $v$, discretizzate in spazio con le stesse funzioni di base di $f$:
$$
g=\sum_{i=1}^N  g_i\varphi_i
$$
$$
v=\sum_{i=1}^N  v_i\varphi_i \qquad v \in \mathbb{R}^N
$$
e si ha:
$$ S_{S_{k,l}} = \int_\Omega \triangle \varphi_k g \ , \ \ \ \ \ \int_\Omega g v = \int_\Omega \triangle \varphi_l v \qquad \forall v \in \mathbb{R}^N$$
Usando la formula di Green ed eliminando gli integrali di bordo grazie alle condizioni di Neumann, si ricava:
$$S_{S_{k,l}} = -\int_\Omega \nabla \varphi_k \nabla g \ , \ \ \ \ \ \int_\Omega g v = -\int_\Omega \nabla \varphi_l \nabla v.$$
Di conseguenza, se si definiscono i vettori con le funzioni di base in spazio e i vettori con le loro derivate parziali
$$
\underline \varphi =
\begin{bmatrix}
\varphi_{1}  \\
\varphi_{2}  \\
\vdots\\
\varphi_{n}
\end{bmatrix}
$$
\begin{equation}
\underline \varphi_x=  \begin{bmatrix}
\partial \varphi_{1}/\partial x \\
\partial \varphi_{2}/\partial x  \\
\vdots\\
\partial \varphi_{n}/\partial x \end{bmatrix} 
\qquad
\underline \varphi_x=  \begin{bmatrix}
\partial \varphi_{1}/\partial y  \\
\partial \varphi_{2}/\partial y  \\
\vdots\\
\partial \varphi_{n}/\partial y\end{bmatrix} 
\end{equation}
e le matrici
$$ R_0 = \int_\Omega \underline \varphi \underline \varphi^T $$
$$ R_1 = \int_\Omega (\underline \varphi_x \underline \varphi_x^T + \underline \varphi_y \underline \varphi_y^T). $$
si ha, per l'arbitrarietà di $v$: 
$$ S_S= - R_1 \underline g \ , \ \ \ \ \ R_0 \underline g = - R_1  $$
dove $\underline g$ contiene i coefficienti dell'espansione in base di $g$. Quindi in conclusione
$$ S_S = R_1 R_0^{-1} R_1 .$$
Per passare all'integrale, però, occorre considerare anche i coefficienti del vettore $\underline c_j =
\begin{bmatrix}
c_{1j} & c_{2j} & \hdots & c_{Nj}
\end{bmatrix}^T$
e si ha:
$$
\int_{\Omega_T} \Bigl( \triangle(\sum_{i=1}^N  c_{ij}\varphi_i ) \Bigr)^2 d \Omega =\underline c^T_j S_S \underline c_j .
$$

Riguardo al termine $\int_0^T (\frac{\partial^2 \sum_{j=1}^M(c_{ij}\psi_j) }{\partial t ^2} )^2 dt$, la semplificazione è molto più semplice, poichè se si considera la matrice
 $$ S_T = \begin{bmatrix}
\int_0^T \psi_1''(t) \psi_1''(t) dt  & \int_0^T \psi_1''(t) \psi_2''(t) dt & \hdots & \int_0^T \psi_1''(t) \psi_M''(t) dt  \\
\int_0^T \psi_2''(t) \psi_1''(t) dt  & \int_0^T \psi_2''(t) \psi_2''(t) dt & \hdots & \int_0^T \psi_2''(t) \psi_M''(t) dt  \\
\vdots & \vdots & \hdots & \vdots \\
\int_0^T \psi_M''(t) \psi_1''(t) dt  & \int_0^T \psi_M''(t) \psi_2''(t) dt & \hdots & \int_0^T \psi_M''(t) \psi_M''(t) dt  \\
\end{bmatrix} .$$
e il vettore $
\underline c_i =
\begin{bmatrix}
c_{i1} & c_{i2} & \hdots & c_{iM}
\end{bmatrix}^T$ allora si ha:
$$
\int_0^T \Bigl( \frac{\partial^2 \sum_{j=1}^M(c_{ij}\psi_j) }{\partial t ^2} \Bigr)^2 dt =\underline c_i^T  S_T \underline c_i
$$

Ora che sono state ricavate le forme quadratiche associate ai due integrali, per completare lo studio di \ref{eq:penalizzdisc} è necessario solamente introdurre opportuni prodotti di Kronacker e l'uso del vettore $\underline c$:
\begin{multline*}
\lambda_S  \sum_{j=1}^M \int_{\Omega_T} \Bigl( \triangle(\sum_{i=1}^N  c_{ij}\varphi_i ) \Bigr)^2 d \Omega + \lambda_T \sum_{i=1}^N\int_0^T \Bigl( \frac{\partial^2 \sum_{j=1}^M(c_{ij}\psi_j) }{\partial t ^2} \Bigr)^2 dt= \\
= \lambda_S\   \underline c^T (I_M \otimes S_S)\  \underline c   \ +\  \lambda_T\  \underline c^T ( S_T \otimes I_N)\  \underline c
\end{multline*}
dove $I_M$ and $I_N$ sono matrici identità di dimensioni $M \times M$ e $N \times N$ rispettivamente.
Di conseguenza se
$$ S = \lambda_S\    (S_S \otimes I_M)   \ +\  \lambda_T\   (I_N \otimes S_T)  .$$
allora
$$
\lambda_S  \sum_{j=1}^M \int_{\Omega_T} \Bigl( \triangle(\sum_{i=1}^N  c_{ij}\varphi_i ) \Bigr)^2 d \Omega + \lambda_T \sum_{i=1}^N\int_0^T \Bigl( \frac{\partial^2 \sum_{j=1}^M(c_{ij}\psi_j) }{\partial t ^2} \Bigr)^2 dt=\underline c^T S \underline c
$$

\subsection*{Soluzione}
Per avere una forma matriciale della versione discreta di $J_{\underline \lambda }(f)$, cioè di 
\begin{multline*}
J_{\underline \lambda }^D(f)= \sum_{i=1}^n \sum_{j=1}^m \bigl( z_{ij} - f(\underline p_i,t_j) \bigr)^2 + \\
+ \lambda_S  \sum_{j=1}^M \int_{\Omega_T} \Bigl( \triangle(\sum_{i=1}^N  c_{ij}\varphi_i ) \Bigr)^2 d \Omega + \lambda_T \sum_{i=1}^N\int_0^T \Bigl( \frac{\partial^2 \sum_{j=1}^M(c_{ij}\psi_j) }{\partial t ^2} \Bigr)^2 dt
\end{multline*}
 occorre definire il vettore $\underline z$ dei valori osservati 
\begin{equation}
\underline z =
\begin{bmatrix}
z_{11}  \\
\vdots\\
z_{1m}  \\
z_{21}  \\
\vdots\\
z_{2m}  \\
\vdots\\
z_{nm}
\end{bmatrix}
\end{equation}
e le matrici $\Phi$ (con le valutazioni delle basi spaziali nei punti $\{\underline p_i; i = 1,...,n\}$) e $\Psi$ (con le valutazioni delle basi temporali $\{t_j; j = 1,...,m\}$):
$$
\Phi =
\begin{bmatrix}
\varphi_{1}(\underline p_1) & \varphi_{2}(\underline p_1) & \hdots & \varphi_{N}(\underline p_1)  \\
\varphi_{1}(\underline p_2) & \varphi_{2}(\underline p_2) & \hdots & \varphi_{N}(\underline p_2)  \\
\vdots & \vdots & \hdots & \vdots \\
\varphi_{1}(\underline p_n) & \varphi_{2}(\underline p_n) & \hdots & \varphi_{N}(\underline p_n)  \\
\end{bmatrix}
$$
$$
\Psi = 
\begin{bmatrix}
\psi_{1}( t_1) & \psi_{2}( t_1) & \hdots & \psi_{M}( t_1)  \\
\psi_{1}( t_2) & \psi_{2}( t_2) & \hdots & \psi_{M}( t_2)  \\
\vdots & \vdots & \hdots & \vdots \\
\psi_{1}( t_n) & \psi_{2}( t_m) & \hdots & \psi_{M}( t_m)  \\
\end{bmatrix}
$$
Sia $\Pi$ il prodotto di Kronecker $\Phi$ e $\Psi$:
$$ \Pi = \Phi \otimes \Psi .$$
Allora
$$
f(\underline p,t)=\sum_{i=1}^n \sum_{j=1}^m c_{ij}\ \varphi_i(\underline p)\ \psi_j(t) = \Pi \underline c .
$$
Quindi si avrà:
\begin{equation} 
\label{eq:Jmatr}
J_{\underline \lambda }^D(f) = (\underline z - \Pi \underline c)^T (\underline z - \Pi \underline c) + \underline c^t S \underline c 
\end{equation}
Una volta che è stata ricavata questa forma, per risolvere il problema di minimo è sufficiente derivare questa espressione, e si ritrova:
$$ \hat  {\underline c} = (\Pi^T \Pi + S)^{-1}\Pi^T \underline z$$



\subsubsection*{Choosing the smoothing parameters}

The values of the smoothing parameters $\lambda_S$ and $\lambda_T$ can be chosen via the minimization of the generalized cross-validation (GCV):
$$ GCV(\underline \lambda) =\frac{nm}{nm-\text{tr}(H)}  D(\hat  {\underline c}) $$
where $\underline \lambda$ is the vector $ (\lambda_S,\lambda_T) $, $nm$ is the number of data, $H$ is the hat matrix that maps the vector of observed values $\underline z$ to the vector of fitted values $\hat  {\underline z}$:
$$ H = \Pi (\Pi^T \Pi + S)^{-1}\Pi^T ,$$
and $D$ is the deviance of the model:
$$  D(\hat  {\underline c}) = (\underline z - \hat  {\underline z})^T(\underline z - \hat  {\underline z}) = (\underline z - H \hat  {\underline c})^T(\underline z - H \hat  {\underline c}).$$


\section{Caso con covariate}

Il modello si estende facilmente se si prevede che il dato possa essere influenzato da covariate. Il modello di (\ref{eq:modellobase}) diventa:
$$ z_{ij}= \underline w_{ij}^T\  \underline \beta   \ + \  f(\underline p_i,t_j)\ +\ \epsilon_{ij}\ \ \ \ i = 1,...,n\ \ j=1,...,m \ \ ,$$
dove $\underline w_{ij}$ è il vettore delle $p$ covariate associate a $z_{ij}$ e $\underline \beta$ è il vettore dei coefficienti di regressione. Di conseguenza, il funzionale discreto di (\ref{eq:Jmatr}) diventa:
$$ J = (\underline z - W \underline \beta - \Pi \underline c)^T (\underline z - W \underline \beta - \Pi \underline c) + \underline c^t S \underline c  \ ,$$
dove $W$ è la matrice $nm \times p$ con i vettori $ \{\underline w_{ij}; i=1,...,n;j=1,...,m\}$.

Per ricavare la soluzione occorre derivare questa espressione rispetto a $\underline \beta$ e $\underline c$:
$$
\frac{\partial}{\partial \underline \beta}J= -2W^T \underline z + 2W^T \Pi \underline c + 2 W^TW \underline \beta \ \ ,
$$
$$
\frac{\partial}{\partial \underline c}J= -2 \Pi^T \underline z + 2 \Pi^T W \underline \beta + 2(\Pi^T \Pi + S) \underline c \ \ .
$$
Imponendo che le derivate siano uguali a zero si hanno le seguenti equazioni:
$$
\begin{cases}
W^TW \hat{\underline \beta} = W^T(\underline z - \Pi \hat{\underline c})  \\
(\Pi^T \Pi + S) \hat{\underline c}=\Pi^T(\underline z -W \hat{\underline \beta})
\end{cases}.
$$
che ricordano le equazioni usate per la regressione e per il modello senza covariate, con la differenza che in questo caso a $\underline z$ è sottratto in entrambi i casi la parte spiegata dal termine di modello a cui non si riferiscono $\hat{\underline \beta}$ e $\hat{\underline c}$ rispettivamente.

\begin{thebibliography}{9}

\bibitem{art:augustin}
Nicole H. Augustin, Verena M. Trenkel, Simon N. Wood, Pascal Lorance, \emph{Space-time modelling of blue ling for fisheries stock management}, Environmetrics, 24, 109–119, (2013)

\bibitem{art:marra}
Giampiero Marra, David L. Miller, Luca Zanin, \emph{Modelling the spatiotemporal distribution of the incidence of resident foreign population}, Statistica Neerlandica, 66, 133–160, (2012)

\bibitem{art:sangalli}
Laura M. Sangalli, James O. Ramsay, Timothy O. Ramsay, \emph{Spatial spline regression models}, Journal of the Royal Statistical Society: Series B, 75, 681–703, (2013)

\bibitem{art:wood}
Simon N. Wood, Mark W. Bravington, Sharon L. Hedley, \emph{Soap film smoothing}, Journal of the Royal Statistical Society: Series B, 70, 931–955, (2008)


%\bibitem{prog:R}
%R Core Team, \emph{R: A Language and Environment for Statistical Computing}, R Foundation for Statistical Computing, Vienna, 2013, \url{http://www.R-project.org/}

\end{thebibliography}

\end{document}



%\[ 
%\underset{\scriptscriptstyle (m\times 1)}{Y}= 
%\underset{\scriptscriptstyle (m\times n)}{X} 
%\underset{\scriptscriptstyle (n\times 1)}{B} 
%\]