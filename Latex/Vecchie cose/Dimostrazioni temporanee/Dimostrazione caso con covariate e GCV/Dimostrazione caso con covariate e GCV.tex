\documentclass[a4paper,12pt]{report}				% COMANDI INIZIALI
\usepackage[italian]{babel}							% sillabazione italiana
\usepackage[utf8]{inputenc}							% Per le lettere accentate IN UNIX E IN WINDOWS
\usepackage{ragged2e}					 			% giustifica
\usepackage{amsmath}								% Per allineare le equazioni
\usepackage{amssymb}								% Per le lettere dell'indicatrice (mathbb)
\usepackage{bm}										% Per le lettere matematiche in grassetto (vettori)

\justifying 										% giustifica

\date{28 Luglio 2014}
\author{Gabriele Mazza}
\title{Dimostrazione caso con covariate}

\begin{document}

%Indice e numerazione
\pagenumbering{arabic}

\chapter{Caso con covariate}

\section{Dimostrazione}
Se si aggiunge il termine di covariate, la risposta sarà modellizzata come:
$$
\underline{z}=W\underline{\beta} + \Pi\underline{c} + \epsilon
$$
La funzione da minimizzare in questo caso sarà:
$$
(\underline{z}-W\underline{\beta}-\Pi\underline{c})^T(\underline{z}-W\underline{\beta}-\Pi\underline{c})+ \underline{c}^TS\underline{c}
$$
Eseguo le moltiplicazioni, e quindi si avrà:
$$
\underline{z}^T\underline{z} -2\underline{z}^TW\underline{\beta} -2\underline{z}^T\Pi\underline{c}
+2\underline{\beta}^TW^T\Pi\underline{c} + \underline{\beta}^TW^TW\underline{\beta}
+\underline{c}(\Pi^T\Pi+S)\underline{c}
$$
Derivo rispetto a $\underline{\beta}$ e $\underline{c}$:
$$
\frac{\partial}{\partial\underline{\beta}}=-2W^T\underline{z} +2W^T\Pi\underline{c} +2W^TW\underline{\beta}
$$
$$
\frac{\partial}{\partial\underline{c}}=-2\Pi^T\underline{z} +2\Pi^TW\underline{\beta} +2(\Pi^T\Pi+S)\underline{c}
$$
Pongo le derivate uguali a zero per avere il punto di minimo:
$$
\frac{\partial}{\partial\underline{\beta}}\Rightarrow
W^TW\underline{\beta} = W^T(\underline{z}-\Pi\underline{c})
$$
$$
\frac{\partial}{\partial\underline{c}}\Rightarrow
(\Pi^T\Pi+S)\underline{c} = \Pi^T(\underline{z}-W\underline{\beta})
$$
che ricordano molto i modelli marginali (regressione classica per la stima di $\underline{\beta}$, e regressione penalizzata per la stima di $\underline{c}$), ma entrambi i casi tolgono a $\underline{z}$ la parte spiegata dall'altra metà di modello.
\newline
Di conseguenza
$$
\underline{\beta}=(W^TW)^{-1}W^T(\underline{z}-\Pi\underline{c})
$$
Sostituisco nell'altra equazione, e si trova (tralascio tutti i conti):
$$
\underline{\hat{c}}=[\Pi^T\Pi+S+\Pi^TW(W^TW)^{-1}W^T\Pi]^{-1}\Pi^T[I-W(W^TW)^{-1}W^T]\underline{z}
$$
Questo valore poi si sostituisce nell'equazione precedente per trovare anche $\underline{\hat{\beta}}$.
La formulazione analitica è complessa, ma alcune parti del calcolo si ripetono (come $W(W^TW)^{-1}W^T$). Quindi è possibile riutilizzare alcune matrici temporanee nel calcolo computazionale della stima.

Inoltre, se si controllano le dimensioni delle matrici, i prodotti tornano. Infatti se si hanno $n$ punti in spazio con dati ($N$ contando anche quelli senza dati, di frontiera), $m$ istanti di tempo (volendo, se si vuole avere una stima temporale più precisa, si può aumentare ad $M$ il numero di basi in tempo), $K$
covariate, si hanno le seguenti dimensioni
$$
\Pi \mbox{ ha dimensione } nm \times NM
$$
$$
S \mbox{ ha dimensione } NM \times NM
$$
$$
W \mbox{ ha dimensione } nm \times K
$$
$$
\underline{z} \mbox{ ha dimensione } nm \times 1
$$
$$
\underline{c} \mbox{ ha dimensione } NM \times 1
$$
$$
\underline{\beta} \mbox{ ha dimensione } K \times 1
$$
e i prodotti matriciali sono coerenti.



\chapter{GCV}

\section{Dimostrazione}
Marra propone di minimizzare la seguente quantità per individuare il miglior $\underline{\lambda}$:
$$
V(\underline{\lambda})=\frac{nm}{(nm-\mbox{tr}(H))^2}D(\underline{\hat{c}})
$$
dove $nm$ è il numero totale di dati, $H$ è la hat matrix che lega $\underline{z}$ a $\underline{\hat{z}}$, $D$ è la devianza, che si calcola come:
$$
D(\underline{c})=2\phi(l_{\mbox{sat}}-l(\underline{\hat{c}}))
$$
dove $\phi$ è il parametro di dispersione della distribuzione di Tweedie con cui si modellizzano i dati (quindi nel caso di distribuzione normale, $\phi=\sigma^2$), $l$ è la logverosimiglianza dei dati.
Viene usata in due modi:
\begin{itemize}
\item $l_{\mbox{sat}}$, cioè valore di saturazione (massimo della logverosimiglianza), con dati usati al posto di parametri
\item $l(\underline{c})$, cioè calcolata con i valori stimati dal modello.
\end{itemize}
Di conseguenza, se si ha il modello (il caso con covariate è perfettamente analogo)
$$
\underline{z}=\Pi\underline{c} + \underline{\epsilon}
$$
con
$$
\underline{\epsilon} \sim N(\underline{0},\sigma^2I)
$$
Allora i dati hanno la seguente distribuzione normale multivariata (ricordo che si hanno $nm$ dati):
$$
\underline{z} \sim N(\Pi\underline{c},\sigma^2I)
$$
e quindi
$$
l_{\mbox{sat}}=\log(\frac{1}{\sqrt{(2\pi\sigma^2)^{nm}}}) -\frac{1}{2\sigma^2}(\underline{z}-\underline{\mu})^T(\underline{z}-\underline{\mu})
$$
Marra indica di usare per il valore di saturazione i dati al posto dei parametri di distribuzione, quindi $\underline{\mu}=\underline{z}$, e infatti la logverosimiglianza assume il valore massimo:
$$
l_{\mbox{sat}}=\log(\frac{1}{\sqrt{(2\pi\sigma^2)^{nm}}})
$$
Invece
$$
l(\underline{\hat{c}})=\log(\frac{1}{\sqrt{(2\pi\sigma^2)^{nm}}}) -\frac{1}{2\sigma^2}(\underline{z}-\Pi\underline{\hat{c}})^T(\underline{z}-\Pi\underline{\hat{c}})
$$
ma $\Pi\underline{\hat{c}}=\underline{\hat{z}}$, quindi
$$
l(\underline{\hat{c}})=\log(\frac{1}{\sqrt{(2\pi\sigma^2)^{nm}}}) -\frac{1}{2\sigma^2}(\underline{z}-\underline{\hat{z}})^T(\underline{z}-\underline{\hat{z}})
$$
Nella differenza $l_{\mbox{sat}}-l(\underline{\hat{c}})$ i termini $\log(\frac{1}{\sqrt{(2\pi\sigma^2)^{nm}}})$ si semplificano.
\newline
\newline
Inserendo questi valori nella formula della devianza, si ha
$$
D(\underline{c})=(\underline{z}-\underline{\hat{z}})^T(\underline{z}-\underline{\hat{z}})
$$
e quindi
$$
V(\underline{\lambda})=\frac{nm}{(nm-\mbox{tr}(H))^2}(\underline{z}-\underline{\hat{z}})^T(\underline{z}-\underline{\hat{z}})
$$
Anche nel caso con covariate si può dimostrare che $V(\underline{\lambda})$ assume lo stesso valore.
\end{document}