\documentclass[a4paper,11pt,twoside,openright]{book}							% COMANDI INIZIALI



\usepackage[italian]{babel}								% sillabazione italiana
\usepackage[utf8]{inputenc}								% Per le lettere accentate IN UNIX E IN WINDOWS

\usepackage[sc]{mathpazo}
%Options: Sonny, Lenny, Glenn, Conny, Rejne, Bjarne, Bjornstrup
\usepackage[Sonny]{fncychap}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{} % cancella tutti i campi
\fancyfoot{}
\fancyhead[RE,LO]{\slshape \leftmark}
\fancyhead[LE,RO]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
%\renewcommand{\footrulewidth}{0.4pt}
\renewcommand{\chaptermark}[1]{%
\markboth{\thechapter.\ #1}{}}

\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{float}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norma}{\lVert}{\rVert}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\listoffigures
\listoftables
\chapter{Descrizione del modello}

In questo capitolo viene descritto nel dettaglio il modello STR-PDE per l'analisi di dati distribuiti in spazio e tempo, ed è calcolata la soluzione al problema di stima.


\section{Caso senza covariate}

\subsection{Dati e modello}

Siano $\{\underline p_i = (x_i,y_i); i=1, \ldots , n\}$ un insieme di $n$ punti spaziali in un dominio limitato $\Omega \subset \mathbb R^2$ e siano $\{t_j ; j=1, \ldots , m\}$ un insieme di $m$ istanti temporali in un intervallo $[0,T]\subset \mathbb R$. In questi punti ed istanti osserviamo i dati: siano quindi $z_{ij}$ i valori della variabile reale nel punto $\underline p_i$ al tempo $t_j$.

Sarebbe possibile trattare i dati come provenienti da funzioni tempo-varianti in ognuno dei punti spaziali considerati, cioè ipotizzare che le osservazioni $\{ z_{ij};i=1, \ldots , n; j=1, \ldots , m \}$ provengano da una funzione $z_i(t)$ valutata all'istante $t_j$. Questa marginalizzazione può analogamente essere eseguita in spazio, supponendo che per ogni istante di tempo esista una funzione variante in spazio definita su $\Omega$ che genera i dati. Tuttavia in questa analisi non si studiano questi modelli marginali, ma si suppone che le osservazioni $\{z_{ij}; i=1, \ldots , n; j=1, \ldots , m\}$ siano generate da un campo campo spazio-temporale con  l'aggiunta di rumore:
\begin{equation}
\label{eq:modellobase}
z_{ij}=f(\underline p_i,t_j)+\varepsilon_{ij}\ \ \ \ i = 1,\ldots,n\ \ j=1,\ldots,m \ \ ,
\end{equation}
dove $\{ \varepsilon_{ij}; i = 1,\ldots ,n; j=1,\ldots m\}$ sono residui indipendenti identicamente distribuiti di media nulla e varianza $\sigma^2$. L'obiettivo del modello STR-PDE sarà la stima della funzione $f(\underline p,t)$ dalle $nm$ osservazioni a disposizione, minimizzando un funzionale $J_{\underline \lambda }(f(\underline p,t))$ con la somma degli scarti quadratici tra le osservazioni e valori stimati e termini di penalizzazione delle derivate per la regolarità della funzione separati in spazio e tempo.



\subsection{Definizione delle funzioni di base per $f(\underline p,t)$}
\label{subs:basi}

Dall'analisi della letteratura già disponibile per modelli simili, anche se solo spaziali, si può dedurre che non è possibile dare una stima della funzione se essa non risulta espressa in una espansione di opportune funzioni di base. Infatti l'infinita possibilità di variazione di una funzione in un qualsiasi spazio funzionale non renderebbe possibile una stima computazionale del risultato. L'approccio scelto per la costruzione di $f(\underline p,t)$ si basa sulla generalizzazione delle espansioni in funzione di base dei casi puramente spaziali o temporali.

Siano 
$$
\{ \varphi_k(t);k=1, \ldots , M \} \subset H^2([T_1,T_2])
$$
un insieme di $M$ funzioni di base definite sull'intervallo temporale $[T_1,T_2]$ e
$$
\{ \psi_l(\underline p);l=1, \ldots , N \} \subset H^1(\Omega)
$$
in insieme di $N$ funzioni di base definite sul dominio spaziale $\Omega$. Con le combinazioni lineari
$$
\sum_{k=1}^M a_k\varphi_k(t) \qquad \sum_{l=1}^N b_l\psi_l(\underline p)
$$
è possibile costruire, rispettivamente, funzioni varianti soltanto in tempo e in spazio. La funzione $f(\underline p,t)$ nasce ipotizzando che i coefficienti costanti delle espansioni in base precedenti possano variare secondo la variabile che non è espressa dalle funzioni di base:
\begin{equation} 
\label{eq:f_temp}
f(\underline p, t) = \sum_{k=1}^M a_k(\underline p)\varphi_k(t)
\end{equation}
\begin{equation}
\label{eq:f_space}
f(\underline p, t) = \sum_{l=1}^N b_l(t)\psi_l(\underline p) \ .
\end{equation}
Si ha quindi che $\{ a_k(\underline p);k=1, \ldots , M \}$ sono i coefficienti spazio-varianti dell'espansione in basi di tempo e $\{ b_l(t);l=1, \ldots , N \}$ sono i coefficienti tempo-varianti dell'espansione in basi spaziali. In questa costruzione la funzione $f(\underline p,t)$ possiede entrambe queste rappresentazioni.

Sono state ricavate due espressioni equivalenti per $f$ ma, come si potrà notare in seguito, questo è coerente e sarà ricavata un'espressione più specifica. Per il momento occorre soltanto aggiungere condizioni per i coefficienti appena introdotti, che saranno necessarie per la costruzione del funzionale di penalizzazione $J_{\underline \lambda }(f(\underline p,t))$:
$$
a_k(\underline p) \in H_{n_0}^2(\Omega) \qquad \forall k=1, \ldots , M
$$
e
$$
b_l(t) \in H^2([T_1,T_2]) \qquad \forall l=1, \ldots , N \ ,
$$
dove $H^2_{n_0}(\Omega) = \{h \in H^2(\Omega) | \partial _{\nu}h=0 \mbox{ su } \partial \Omega\}$, spazio incluso in $H^2(\Omega)$ che garantisce alle funzioni $a_k(\underline p)$ di avere condizioni di Neumann alla frontiera.


\subsection{Funzionale di penalizzazione $J_{\underline \lambda }(f(\underline p,t)))$}

Per poter stimare $f(\underline p,t)$ si introduce la minimizzazione di un funzionale non formato solamente dagli scarti quadratici tra le osservazioni e le stime negli $nm$ punti disponibili. Sono inclusi in esso anche altri due termini, che derivano dalla penalizzazione di opportune derivate in spazio e tempo per poter garantire regolarità alla funzione.

Analogamente a quanto fatto il \ref{subs:basi}, per costruire tale funzionale si considerano inanzitutto i problemi marginali in spazio e tempo.

Per una funzione spazio-variante sono disponibili più alternative per penalizzare la regolarità, ma in questo caso si considera il quadrato della norma $L^2$ del laplaciano (dove per laplaciano si intenderà, da ora in avanti, rispetto alle variabili in spazio $\underline p$). La stessa scelta è stata fatta in altre pubblicazioni come \cite{art:ramsay}, \cite{art:sangalli} e \cite{art:wood}. Quindi se $g(\underline p): \Omega \mapsto \mathbb{R}$ è una funzione spazio-variante, allora si può definire la penalizzazione della regolarità in spazio tramite:
$$
J_S\left(g(\underline p)\right)=\int_{\Omega} \Bigl( \Delta  g(\underline p  ) \Bigr)^2 d \underline p \ .
$$
Analogamente in tempo si avrà la penalizzazione del quadrato della norma $L^2$ della derivata seconda. Se $h(t): [T_1,T_2] \mapsto \mathbb{R}$, allora si avrà
$$
J_T\left(h(t)\right)=\int_{T_1}^{T_2} \Bigl( \frac{\partial^2   h(t)   }{\partial t ^2} \Bigr)^2 dt\ .
$$
Grazie alle ipotesi di regolarità introdotte in sez. \ref{subs:basi} tali penalizzazioni possono essere applicate ai coefficienti degli sviluppi delle eq. \ref{eq:f_temp} e \ref{eq:f_space}. Per questo motivo si definisce:
\begin{multline*}
J_{\underline \lambda }(f(\underline p,t))=\sum_{i=1}^n \sum_{j=1}^m \bigl( z_{ij} - f(\underline p_i,t_j) \bigr)^2 \ + \\
+\lambda_S  \sum_{k=1}^M J_S\Bigl( a_k(\underline p)\Bigr) + \lambda_T \sum_{l=1}^N J_T \Bigl( b_l(t)\Bigr) \ ,
\end{multline*}
cioè
\begin{multline}
\label{eq:penalizzdisc}
J_{\underline \lambda }(f(\underline p,t))=\sum_{i=1}^n \sum_{j=1}^m \bigl( z_{ij} - f(\underline p_i,t_j) \bigr)^2 \ + \\
+\lambda_S  \sum_{k=1}^M \int_{\Omega} \Bigl( \Delta(  a_k(\underline p)  ) \Bigr)^2 d \underline p + \lambda_T \sum_{l=1}^N\int_{T_1}^{T_2} \Bigl( \frac{\partial^2   b_l(t)   }{\partial t ^2} \Bigr)^2 dt \ ,
\end{multline}
dove $\lambda_S>0$ e $\lambda_T>0$ sono i parametri di smoothing che stabiliscono il peso della penalizzazione della regolarità della funzione rispettivamente in spazio e tempo. Se troppo alti, la funzione stimata tenderà ad essere quasi liscia e distante dai dati. Al contrario, se troppo bassi, la funzione stimata sarà quasi l'interpolazione dei dati e per nulla liscia. Sebbene quest'ultimo caso si avvicini molto ai valori osservati, non può essere considerato buono perchè solitamente attribuisce alla stima troppe variazioni repentine e quindi non è adatta a descrivere il fenomeno.



\subsection{Modello ideale}
Il funzionale di penalizzazione riportato in eq. \ref{eq:penalizzdisc} è il risultato di una costruzione associata alla generalizzazione delle penalizzazioni marginali in spazio e tempo, ma idealmente si può ricondurre al seguente:
\begin{multline}
\label{eq:Jfunc_cont}
\tilde J_{\underline \lambda }(f) = \sum_{i=1}^n \sum_{j=1}^m \bigl( z_{ij} - f(\underline p_i,t_j) \bigr)^2 \ + \\+\   \lambda_S \int_{T_1}^{T_2} \int_\Omega \Bigl( \Delta f(\underline p, t)  \Bigr)^2 d\underline p\ dt \ +\  \lambda_T \int_\Omega \int_{T_1}^{T_2} \Bigl( \frac{\partial^2 f(\underline p, t) }{\partial t ^2} \Bigr)^2 dt\ d\underline p ,
\end{multline}
dove $J_S$ e $J_T$ sono applicati direttamente alla funzione $f(\underline p, t)$ e sono integrati, rispettivamente, sull'intervallo spaziale e il dominio spaziale.

Se si applica la forma di $f(\underline p,t)$ dell'eq. \ref{eq:f_temp} nel termine di penalizzazione del laplaciano in spazio, allora si può ritrovare:
\begin{equation} 
\label{eq:espansione_funzinf_t}
\begin{split}
\int_{T_1}^{T_2} \int_\Omega \Bigl( \Delta f(\underline p, t)  \Bigr)^2 d\underline p\ dt 
&=\int_{T_1}^{T_2} \int_\Omega \Bigl( \Delta \bigl( \sum_{k=1}^M a_k(\underline p) \varphi_k(t) \bigr)  \Bigr)^2 d\underline p\ dt \\
&=\int_{T_1}^{T_2} \int_\Omega \Bigl( \sum_{k=1}^M \Delta a_k(\underline p) \varphi_k(t)  \Bigr)^2 d\underline p\ dt \\
&=\int_{T_1}^{T_2} \int_\Omega \Bigl( \sum_{k=1}^M \Delta a_k(\underline p) \varphi_k(t)  \Bigr)\Bigl( \sum_{h=1}^M \Delta a_h(\underline p) \varphi_h(t)  \Bigr) d\underline p\ dt \\
&=\int_{T_1}^{T_2} \int_\Omega \Bigl( \sum_{k=1}^M \sum_{h=1}^M \Delta a_k(\underline p)\Delta a_h(\underline p)\ \varphi_k(t)  \varphi_h(t)  \Bigr) d\underline p\ dt \\
&=\sum_{k=1}^M \sum_{h=1}^M \int_\Omega   \Delta a_k(\underline p) \Delta a_h(\underline p)d\underline p \ \int_{T_1}^{T_2} \varphi_k(t)\varphi_h(t)   \ dt .
\end{split}
\end{equation}
Questo termine è equivalente a quello proposto in \ref{eq:penalizzdisc} se le basi temporali siano ortonormali, poichè in tal caso l'ultimo integrale in eq. \ref{eq:espansione_funzinf_t} sarebbe 1 se $k=h$ e 0 altrimenti.

Allo stesso modo, se si sostituisce la forma di $f(\underline p,t)$ dell'eq. \ref{eq:f_space} nella penalizzazione ideale dell'eq. \ref{eq:Jfunc_cont} si ottiene:
\begin{equation} 
\label{eq:espansione_funzinf_sp}
\begin{split}
\int_\Omega \int_{T_1}^{T_2} \Bigl( \frac{\partial^2 f(\underline p, t) }{\partial t ^2} \Bigr)^2 dt\ d\underline p 
&=\int_\Omega \int_{T_1}^{T_2} \Bigl( \frac{\partial^2 \sum_{l=1}^N b_l(t)\psi_l(\underline p) }{\partial t ^2} \Bigr)^2 dt\ d\underline p \\
&=\int_\Omega \int_{T_1}^{T_2} \Bigl( \sum_{l=1}^N \frac{ \partial^2b_l(t)}{\partial t ^2}\psi_l(\underline p)  \Bigr)^2 dt\ d\underline p  \\
&=\int_\Omega \int_{T_1}^{T_2} \Bigl( \sum_{l=1}^N \frac{ \partial^2b_l(t)}{\partial t ^2}\psi_l(\underline p)  \Bigr)\Bigl( \sum_{h=1}^N \frac{ \partial^2b_h(t)}{\partial t ^2}\psi_h(\underline p)  \Bigr) dt\ d\underline p \\
&= \int_\Omega \int_{T_1}^{T_2} \Bigl( \sum_{l=1}^N \sum_{h=1}^N \frac{ \partial^2b_l(t)}{\partial t ^2}\frac{ \partial^2b_h(t)}{\partial t ^2}\   \psi_l(\underline p)  \psi_h(\underline p)  \Bigr) dt\ d\underline p \\
&=\sum_{l=1}^N \sum_{h=1}^N  \int_{T_1}^{T_2}  \frac{ \partial^2b_l(t)}{\partial t ^2}\frac{ \partial^2b_h(t)}{\partial t ^2}dt\  \int_\Omega \psi_l(\underline p)  \psi_h(\underline p)    d\underline p .
\end{split}
\end{equation}
La stessa osservazione del caso precedente vale anche ora: se le basi in spazio sono ortonormali, si ritrova la penalizzazione proposta in \ref{eq:penalizzdisc}. 

In questo lavoro di tesi, quindi, è proposto un modello che risulterà essere computazionalmente semplice ma non perfettamente equivalente a questo modello ideale. Le basi proposte, infatti, non sono ortonormali. Comunque sia, gli insiemi di basi che saranno adattati sono sparsi, cioè i termini $ \int_{T_1}^{T_2} \varphi_k(t)\varphi_l(t)\ dt $ e $\int_\Omega \psi_l(\underline p)  \psi_k(\underline p)d\underline p$ sono diversi da zero solo per poche coppie di indici.






\subsection*{Discretizzazione dei termini di penalizzazione delle derivate}
Così come è scritto in \ref{eq:penalizzdisc}, il funzionale $J_{\underline \lambda }(f(\underline{p},t))$ non è ancora adatto ad essere trattato computazionalmente. Per poter avere una forma che renda semplice la stima, $f(\underline{p},t)$ e $J_{\underline \lambda }(f(\underline{p},t))$ saranno nuovamente discretizzati. 

Il primo caso da trattare è l'integrale del laplaciano dei coefficienti spazio-varianti dell'eq. \ref{eq:f_temp} in modo analogo a quanto fatto in \cite{art:sangalli}. Fissato $k$, l'integrale
$$
\int_{\Omega} \Bigl( \Delta(  a_k(\underline p)  ) \Bigr)^2 d \underline p
$$
può essere semplificato introducendo la funzione $g_k(\underline p)$ come segue:
\begin{equation}
\label{eq:ps1}
\int_\Omega g_k(\underline{p}) v(\underline{p}) d \Omega= \int_\Omega \Delta (a_k(\underline p)) v(\underline p) d \Omega\qquad \forall v(\underline{p}) \in L^2(\Omega) \ .
\end{equation}
Non è difficile verificare che, se $g_k(\underline p)$ rispetta l'equazione precedente, per l'arbitrarietà di $v$ allora:
\begin{equation}
\label{eq:ps2}
\int_{\Omega} \Bigl( \Delta(  a_k(\underline p)  ) \Bigr)^2 d \underline p = \int_{\Omega}  \Delta(  a_k(\underline p))g_k(\underline p)d \underline p \ .
\end{equation}
Applicando la la formula di Green e tenendo conto delle condizioni di Neumann per $a_k(\underline p)$, si possono semplificare gli integrali come segue:
$$
\int_{\Omega}  \Delta(  a_k(\underline p))g_k(\underline p)d \underline p \ = -\int_{\Omega} \nabla  a_k(\underline p)\nabla g_k(\underline p)d \underline p
$$
$$
\int_{\Omega}  \Delta(  a_k(\underline p))v(\underline p)d \underline p \ = -\int_{\Omega} \nabla  a_k(\underline p)\nabla v(\underline p)d \underline p \ .
$$
Per poter calcolare analiticamente questi integrali è necessario introdurre l'uso delle basi spaziali $\{ \psi_l(\underline p);l=1, \ldots , N \}$ per le funzioni $a_k$, $g_k$ e $v$. Siano quindi:
$$
a_k(\underline p)=\sum_{l=1}^N c_{lk}\psi_l(\underline p) \qquad 
g_k(\underline p)=\sum_{l=1}^N g_{lk}\psi_l(\underline p) \qquad
v(\underline p)=\sum_{l=1}^N v_{l}\psi_l(\underline p) \ .
$$
Per semplificare le notazioni saranno usati i seguenti vettori:
$$
\underline c_k =
\begin{bmatrix}
c_{1k} \\ c_{2k} \\ \hdots \\ c_{Nk}
\end{bmatrix}
\qquad
\underline g_k =
\begin{bmatrix}
g_{1k} \\ g_{2k} \\ \hdots \\ g_{Nk}
\end{bmatrix}
\qquad
\underline v =
\begin{bmatrix}
v_1 \\ v_2 \\ \hdots \\ v_N
\end{bmatrix} 
$$
e gli analoghi per le funzioni di base e le loro derivate parziali:
$$
\underline \psi =
\begin{bmatrix}
\psi_{1}  \\
\psi_{2}  \\
\vdots\\
\psi_{n}
\end{bmatrix}
$$
\begin{equation}
\underline \psi_x=  \begin{bmatrix}
\partial \psi_{1}/\partial x \\
\partial \psi_{2}/\partial x  \\
\vdots\\
\partial \psi_{n}/\partial x \end{bmatrix} 
\qquad
\underline \psi_x=  \begin{bmatrix}
\partial \psi_{1}/\partial y  \\
\partial \psi_{2}/\partial y  \\
\vdots\\
\partial \psi_{n}/\partial y\end{bmatrix} \ .
\end{equation}
Mediante l'uso delle funzioni di base e di ciò che è stato ottenuto dall'applicazione della formula di Green, le relazioni \ref{eq:ps1} e \ref{eq:ps2} diventano:
$$
\underline{g}_k \Bigl(\int_\Omega \underline \psi \underline \psi^T \Bigr)\underline{v}=
-\underline{c}_k \Bigl(\int_\Omega (\underline \psi_x \underline \psi_x^T + \underline \psi_y \underline \psi_y^T)\Bigr) \underline{v} \qquad \forall \underline{v} \in \mathbb{R}^N
$$
$$
\int_{\Omega} \Bigl( \Delta(  a_k(\underline p)  ) \Bigr)^2 d \underline p = -\underline{c}_k \Bigl(\int_\Omega (\underline \psi_x \underline \psi_x^T + \underline \psi_y \underline \psi_y^T) \Bigr)\underline{g}_k \ ,
$$
quindi, se si introducono le matrici (analogamente a quanto fatto in \cite{art:sangalli})
$$ R_0 = \int_\Omega \underline \psi \underline \psi^T $$
$$ R_1 = \int_\Omega (\underline \psi_x \underline \psi_x^T + \underline \psi_y \underline \psi_y^T) \ $$
si trova, per l'arbitrarietà di $\underline v$:
\begin{equation}
\label{eq:pspace}
\int_{\Omega} \Bigl( \Delta(  a_k(\underline p)  ) \Bigr)^2 d \underline p = \underline{c}_k^T R_1 R_0^{-1} R_1 \underline{c}_k=\underline{c}_k^T P_S \underline{c}_k
\end{equation}
Si noti che la matrice $P_S$ non dipende da $k$ e può essere considerata la stessa per tutte le funzioni $a_k(\underline{p})$. Inoltre è simmetrica, poichè $R_0$ e $R_1$ lo sono.

Grazie all'introduzione della discretizzazione in basi spaziali
$$
a_k(\underline p)=\sum_{l=1}^N c_{lk}\psi_l(\underline p)
$$
è stato possibile ridurre la penalizzazione con l'integrale del quadrato di $\Delta a_k(\underline p)$
alla valutazione di una forma quadratica che non cambia con le funzioni $a_k$. Si ha anche un'altra conseguenza: per l'equivalenza ipotizzata tra le espressioni di $f(\underline{p},t)$ in \ref{eq:f_temp} e \ref{eq:f_space}, allora è necessario che i coefficienti tempo-varianti dell'espansione in basi spaziali assumano la seguente forma:
$$
b_l(t)=\sum_{k=1}^M c_{lk}\varphi_k(t) \ .
$$
Si ritrova quindi anche in questo caso l'espansione in funzioni di base.

Non resta altro che discretizzare anche $\int_{T_1}^{T_2} \Bigl( \frac{\partial^2   b_l(t)   }{\partial t ^2} \Bigr)^2 dt$. Dopo aver introdotto l'uso delle funzioni di base, se si definisce 
 $$ P_T = \begin{bmatrix}
\int_{T_1}^{T_2} \varphi_1''(t) \varphi_1''(t) dt  & \int_{T_1}^{T_2} \varphi_1''(t) \varphi_2''(t) dt & \hdots & \int_{T_1}^{T_2} \varphi_1''(t) \varphi_M''(t) dt  \\
\int_{T_1}^{T_2} \varphi_2''(t) \varphi_1''(t) dt  & \int_{T_1}^{T_2} \varphi_2''(t) \varphi_2''(t) dt & \hdots & \int_{T_1}^{T_2} \varphi_2''(t) \varphi_M''(t) dt  \\
\vdots & \vdots & \hdots & \vdots \\
\int_{T_1}^{T_2} \varphi_M''(t) \varphi_1''(t) dt  & \int_{T_1}^{T_2} \varphi_M''(t) \varphi_2''(t) dt & \hdots & \int_{T_1}^{T_2} \varphi_M''(t) \varphi_M''(t) dt  \\
\end{bmatrix} $$
e il vettore $$
\underline{c}_l =
\begin{bmatrix}
c_{l1} \\ c_{l2} \\ \hdots \\ c_{lM}
\end{bmatrix} \ ,$$ si ritrova:
$$
\int_{T_1}^{T_2} \Bigl( \frac{\partial^2   b_l(t)   }{\partial t ^2} \Bigr)^2 dt = \underline{c}_l^T  P_T \underline{c}_l \ .
$$
Anche la matrice $P_T$ è simmetrica.

In conclusione si può notare come la parte di penalizzazione per la regolarizzazione di $f$ in \ref{eq:penalizzdisc} sia diventata un'unica forma quadratica. Per mostrarlo, si introduce il vettore
$$\underline c =
\begin{bmatrix}
c_{11}  \\
\vdots\\
c_{1M}  \\
c_{21}  \\
\vdots\\
c_{2M}  \\
\vdots\\
c_{NM}
\end{bmatrix}
$$
e la matrice $P$, definita con opportuni prodotti di Kronecker come segue:
$$
P = \lambda_S\    (P_S \otimes I_M)   \ +\  \lambda_T\   (I_N \otimes P_T) \ ,
$$
dove $I_M$ and $I_N$ sono matrici identità di dimensioni $M \times M$ e $N \times N$ rispettivamente. Allora si avrà:
\begin{multline}
\lambda_S  \sum_{k=1}^M \int_{\Omega} \Bigl( \Delta(  a_k(\underline p)  ) \Bigr)^2 d \underline p + \lambda_T \sum_{l=1}^N\int_{T_1}^{T_2} \Bigl( \frac{\partial^2   b_l(t)   }{\partial t ^2} \Bigr)^2 dt 
\\ \lambda_S\sum_{k=1}^M\underline{c}_k^T P_S \underline{c}_k + \lambda_T\sum_{l=1}^N\underline{c}_l^T P_T \underline{c}_l = \underline{c}^T P \underline{c} \ .
\end{multline}
A causa della simmetria dei termini con cui è costruita, anche la matrice $P$ è simmetrica.


\subsection{Soluzione del problema di stima}
Grazie a quanto ricavato nel paragrafo precedente, la parte di penalizzazione delle derivate del funzionale $J_{\underline \lambda }(f(\underline p,t))$ si è ridotta ad un'unica forma quadratica. Ma questo è stato possibile grazie all'espressione in funzione di base per i coefficienti delle eq. \ref{eq:f_temp} e \ref{eq:f_space}:
$$
a_k(\underline p)=\sum_{l=1}^N c_{lk}\psi_l(\underline p) \qquad b_l(t)=\sum_{k=1}^M c_{lk}\varphi_k(t) \ .
$$
Essendo \ref{eq:f_temp} e \ref{eq:f_space} equivalenti, allora in definitiva:
\begin{equation} 
\label{eq:basisexp}
f(\underline p,t)=\sum_{l=1}^N \sum_{k=1}^M c_{lk}\ \psi_l(\underline p)\ \varphi_k(t) ,
\end{equation}
cioè la funzione da stimare è la combinazione lineare di tutti i possibili prodotti incrociati tra le funzioni di base in tempo e spazio. Questa formulazione può essere considerata la definitiva per la funzione $f(\underline p,t)$ e permette di poter identificare la funzione con il vettore dei suoi coefficienti $\underline{c}$. Inoltre ne consegue che è possibile scrivere in modo più agevole il funzionale $J_{\underline \lambda }(f(\underline p,t))$.

Siano definiti il vettore dei valori osservati
\begin{equation}
\underline z =
\begin{bmatrix}
z_{11}  \\
\vdots\\
z_{1m}  \\
z_{21}  \\
\vdots\\
z_{2m}  \\
\vdots\\
z_{nm}
\end{bmatrix}
\end{equation}
e le matrici $\Psi$ (con le valutazioni delle basi spaziali nei punti $\{\underline p_i; i = 1,\ldots,n\}$) e $\Phi$ (con le valutazioni delle basi temporali $\{t_j; j = 1,\ldots,m\}$):
$$
\Psi =
\begin{bmatrix}
\psi_{1}(\underline p_1) & \psi_{2}(\underline p_1) & \hdots & \psi_{N}(\underline p_1)  \\
\psi_{1}(\underline p_2) & \psi_{2}(\underline p_2) & \hdots & \psi_{N}(\underline p_2)  \\
\vdots & \vdots & \hdots & \vdots \\
\psi_{1}(\underline p_n) & \psi_{2}(\underline p_n) & \hdots & \psi_{N}(\underline p_n)  \\
\end{bmatrix}
$$
$$
\Phi = 
\begin{bmatrix}
\varphi_{1}( t_1) & \varphi_{2}( t_1) & \hdots & \varphi_{M}( t_1)  \\
\varphi_{1}( t_2) & \varphi_{2}( t_2) & \hdots & \varphi_{M}( t_2)  \\
\vdots & \vdots & \hdots & \vdots \\
\varphi_{1}( t_n) & \varphi_{2}( t_m) & \hdots & \varphi_{M}( t_m)  \\
\end{bmatrix} \ .
$$
Le ultime due matrici hanno una grossa utilità se moltiplicate tra loro con prodotto di Kronecker, poichè se
$$ B = \Psi \otimes \Phi \ ,$$
allora si può facilmente dire che:
$$
\begin{bmatrix}
f(\underline p_1,t_1)  \\
\vdots\\
f(\underline p_1,t_m)  \\
f(\underline p_2,t_1)  \\
\vdots\\
f(\underline p_2,t_m)  \\
\vdots\\
f(\underline p_n,t_m)
\end{bmatrix}= B \underline c \ .
$$

Quindi è possibile dare una forma definitiva al funzionale di penalizzazione:
\begin{equation} 
\label{eq:Jmatr}
J_{\underline \lambda }(\underline c) = (\underline z - B \underline c)^T (\underline z - B \underline c) + \underline c^T P \underline c \ ,
\end{equation}
e per trovare la stima della funzione $f(\underline{p},t)$ sarà sufficiente ricavare il vettore dei coefficienti $\underline c$ risolvendo il problema di minimo:
$$
\hat{\underline{c}}=\argmin_{c \in \mathbb{R}^{NM}} J_{\underline \lambda }(\underline c) \ .
$$
Grazie alla formulazione ottenuta in \ref{eq:Jmatr} basta derivare per ottenere la soluzione al problema di stima. Grazie alla simmetria di $P$, si ha:
$$
\frac{\partial}{\partial \underline c}J= -2 B^T \underline z + 2(B^T B + P) \underline c \ \ ,
$$
che posta uguale a zero porta all'equazione
$$
(B^T B + P) \underline c = B^T\underline z
$$ 
e in conclusione
$$ \hat  {\underline c} = (B^T B + P)^{-1}B^T \underline z \ .$$

Il problema di stima è stato risolto e la soluzione si ricava dalla risoluzione di un sistema lineare, seppur di grandi dimensioni (la matrice $B^T B + P$ ha dimensioni $NM \times NM$ e già dagli esempi si potranno notare dimensioni elevate).

L'ultimo elemento da definire è la \textit{smoothing matrix} $S$, usata per conoscere i valori stimati dal modello direttamente da quelli osservati:
$$
\hat  {\underline z} =B\hat  {\underline c} = B(B^T B + P)^{-1}B^T \underline z = S\underline{z} \ .
$$



\subsection{Proprietà statistiche di $\hat  {\underline c}$}
Il modello di partenza indicato in \ref{eq:modellobase} può essere scritto anche in forma matriciale
\begin{equation}
\label{eq:modellobasematric}
\underline z=B \underline c + \underline \varepsilon .
\end{equation}
A causa delle proprietà statistiche di $\underline \varepsilon$
$$
\mathbb{E}[\underline \varepsilon] = \underline 0 \qquad \mathrm{Var}[\underline \varepsilon] = \sigma^2 I_{nm}
$$
sia ha
$$
\mathbb{E}[\underline z] = B \underline c \qquad \mathrm{Var}[\hat  {\underline c}] = \sigma^2 I_{nm}
$$
e quindi è immediato ricavare per lo stimatore $\hat  {\underline c}$ (grazie alle proprietà simmetria di $P$):
$$
\mathbb{E}[\hat  {\underline c}] = (B^T B + P)^{-1}B^TB \underline c \qquad \mathrm{Var}[\underline z] = \sigma^2 (B^T B + P)^{-1}B^TB(B^T B + P)^{-1} .
$$
Non è stata ipotizzata la gaussianità per $\underline \varepsilon$ ma se fosse ipotizzata anche $\hat  {\underline c}$ sarebbe gaussiano. Attraverso questa ulteriore ipotesi sarebbe possibile elaborare (con una data significatività) una regione di confidenza per $\hat  {\underline c}$ e quindi una banda di confidenza per la funzione stimata $f$.


\section{Caso con covariate}

Il modello si estende facilmente se si prevede che il dato possa essere influenzato da covariate. Il modello di (\ref{eq:modellobase}) diventa:
\begin{equation}
\label{eq:modellobasecovar}
z_{ij}= \underline w_{ij}^T\  \underline \beta   \ + \  f(\underline p_i,t_j)\ +\ \varepsilon_{ij}\ \ \ \ i = 1,\ldots,n\ \ j=1,\ldots,m \ \ ,
\end{equation}
dove $\underline w_{ij}$ è il vettore delle $p$ covariate associate a $z_{ij}$ e $\underline \beta$ è il vettore dei coefficienti di regressione. Di conseguenza il funzionale discreto di \ref{eq:Jmatr} diventa:
$$ J_{\underline \lambda }(\underline c) = (\underline z - W \underline \beta - B \underline c)^T (\underline z - W \underline \beta - B \underline c) + \underline c^t S \underline c  \ ,$$
dove $W$ è la matrice $nm \times p$ con i vettori $ \{\underline w_{ij}; i=1,\ldots,n;j=1,\ldots,m\}$.

Per trovare la soluzione occorre derivare questa espressione rispetto a $\underline \beta$ e $\underline c$:
$$
\frac{\partial}{\partial \underline \beta}J= -2W^T \underline z + 2W^T B \underline c + 2 W^TW \underline \beta \ \ ,
$$
$$
\frac{\partial}{\partial \underline c}J= -2 B^T \underline z + 2 B^T W \underline \beta + 2(B^T B + P) \underline c \ \ .
$$
Imponendo che le derivate siano uguali a zero si hanno le seguenti equazioni:
$$
\begin{cases}
W^TW \hat{\underline \beta} = W^T(\underline z - B \hat{\underline c})  \\
(B^T B + P) \hat{\underline c}=B^T(\underline z -W \hat{\underline \beta})
\end{cases}.
$$
che ricordano le equazioni usate per la regressione e per il modello senza covariate, con la differenza che in questo caso a $\underline z$ è sottratto, in entrambi i casi, la parte spiegata dal termine di modello a cui non si riferiscono $\hat{\underline \beta}$ e $\hat{\underline c}$ rispettivamente.

A questo punto si possono ricavare le soluzioni. Si ha:
$$
\hat  {\underline c}=[B^TB+P+B^TW(W^TW)^{-1}W^TB]^{-1}B^T[I-W(W^TW)^{-1}W^T]\underline z=AQ \underline z
$$
con $A=[B^TB+P+B^TW(W^TW)^{-1}W^TB]^{-1}B^T$ e $Q=[I-W(W^TW)^{-1}W^T]$, matrice molto importante nel caso di regressione lineare, poiché essa proietta il vettore dei dati nel sottospazio ortogonale allo spazio generato dalle colonne della matrice disegno, ricavando così il vettore dei residui. Questa matrice si ritrova anche in questo caso, e sono valide le sue proprietà:
\begin{itemize}
\item $Q$ è idempotente, cioè $QQ=Q$;
\item $Q$ è simmetrica;
\item a causa del fatto che proietta nel sottospazio ortogonale di $\mathrm{Col}(W)$, $QW$ risulta essere la matrice nulla di opportune dimensioni. 
\end{itemize}

Infine, la stima di $\hat  {\underline \beta}$ si ottiene dalla stima ottenuta per $\hat  {\underline c}$:
$$
\hat{\underline{\beta}}(W^TW)^{-1}W^T(I-B AQ)\underline z
$$

In modo analogo al caso senza covariate, è necessario ricavare anche per questo caso la \textit{smoothing matrix}:
$$
\hat  {\underline z} =B\hat  {\underline c} + W \hat  {\underline \beta} = [B AQ + W(W^TW)^{-1}W^T(I-B AQ)]\underline z = S\underline z .
$$

\subsection{Proprietà statistiche di $\hat  {\underline c}$ e $\hat  {\underline \beta}$}
Anche in questo caso è possibile calcolare valore atteso e varianza degli stimatori ottenuti, ed è utile in quanto consente di verificare la significatività delle covariate. Per farlo, però, è necessario avere la forma matriciale del modello indicato in \ref{eq:modellobasecovar}:

\begin{equation}
\label{eq:modellobasecovarmatric}
\underline z=B \underline c + W \underline \beta + \underline \varepsilon .
\end{equation}
Di nuovo si ha 
$$
\mathbb{E}[\underline \varepsilon] = \underline 0 \qquad \mathrm{Var}[\underline \varepsilon] = \sigma^2 I_{nm}
$$
e di conseguenza
$$
\mathbb{E}[\underline z] = B \underline c + W \underline \beta \qquad \mathrm{Var}[\underline z] = \sigma^2 I_{nm} .
$$
Mediate questo risultato e le proprietà ricavate per la matrice $Q$ è possibile ottenere che:
$$
\mathbb{E}[\hat  {\underline c}] = AQB \underline c \qquad \mathrm{Var}[\hat  {\underline c}] = \sigma^2 AQA^T .
$$
Per $\hat  {\underline \beta}$ i calcoli sono più complessi, ma si semplificano grazie alle proprietà indicate in precedenza per la matrice $Q$. Si ritrova:
$$
\mathbb{E}[\hat  {\underline \beta}] = \underline \beta + (W^TW)^{-1}W^T(I-B AB)\underline c
$$
$$ \mathrm{Var}[\hat  {\underline \beta}] = \sigma^2 (W^TW)^{-1} + \sigma^2 (W^TW)^{-1}W^T B A Q A^T B^T W(W^TW)^{-1}.
$$

Come nel caso senza covariate, anche ora si potrebbe stimare la gaussianità degli stimatori se fosse ipotizzata per $\underline \varepsilon$. Questo permette di elaborare intervalli di confidenza per le componenti di $\underline \beta$ e di verificare la signiificatività delle covariate.


\section{Stima di $\sigma^2$ e scelta dei parametri $\lambda_S$ e $\lambda_T$}
\label{sez:GCV}

Quanto riportato di seguito è valido indipendentemente dal fatto che siano inserite nel modello le covariate, quindi per entrambi i modelli proposti in precedenza.

\subsection{Stima di $\sigma^2$}
Stimare la varianza dell'errore è necessario se si vuole fare inferenza sugli stimatori ed è molto semplice se si conoscono i gradi di libertà equivalenti del modello. Ma questi si ricavano dalla \textit{smoothing matrix}:
$$
\mathrm{EDF}=\mathrm{tr}(S) \ .
$$
Con questo valore si calcola la stima della varianza, usando i residui e il numero totale di dati:
$$
\hat{\sigma}^2=\frac{1}{nm-tr(S)}(\underline z - \hat  {\underline z})^T(\underline z - \hat  {\underline z})
$$

\subsection{Parametri $\lambda_S$ e $\lambda_T$}
I parametri $\lambda_S$ e $\lambda_T$ hanno un ruolo rilevante nella stima della soluzione, poichè scelgono quanto peso dare alla regolarità della funzione in spazio e tempo. Quindi è opportuno che siano fissati accuratamente prima della stima della soluzione.

Secondo quanto indicato in \cite{art:marra}, la scelta corretta si ha con il valore di $\underline \lambda$ che realizza il minimo dell'indice di \textit{generalized cross validation}
$$
GCV(\underline \lambda) =\frac{nm}{nm-\text{tr}(S)}  D(\hat  {\underline c},\hat  {\underline \beta}) \ ,
$$
dove $D$ è la devianza del modello. Si ha:
$$
D(\hat  {\underline c},\hat  {\underline \beta})=2\sigma^2(l_{\mathrm{sat}}-l(\hat  {\underline c},\hat  {\underline \beta})) \ ,
$$
dove $l$ è la logverosimiglianza del modello, che si ipotizza gaussiano, valutata rispettivamente nel suo massimo (valore di saturazione) e in corrispondenza dei valori stimati. Non è difficile dimostrare che, sia nel caso con covariate che senza covariate, si ha: 
$$
D(\hat  {\underline c},\hat  {\underline \beta}) = (\underline z - \hat  {\underline z})^T(\underline z - \hat  {\underline z})
$$
Di conseguenza, il miglior $\underline \lambda$ può essere scelto come valore che minimizza
$$
GCV(\underline \lambda) =\frac{nm}{nm-\text{tr}(S)}  (\underline z - \hat  {\underline z})^T(\underline z - \hat  {\underline z}) \ .
$$


\begin{thebibliography}{9}

\bibitem{art:augustin}
Nicole H. Augustin, Verena M. Trenkel, Simon N. Wood, Pascal Lorance, \emph{Space-time modelling of blue ling for fisheries stock management}, Environmetrics, 24, 109–119, (2013)

\bibitem{art:azzimonti}
Laura Azzimonti, Laura M. Sangalli, Piercesare Secchi, Maurizio Domanin, Fabio Nobile, \emph{Blood flow velocity field estimation via spatial regression with PDE penalization}, Journal of the American Statistical Association, (2015) DOI: 10.1080/01621459.2014.946036

\bibitem{art:gcv}
Peter Craven, Grace Wahba, \emph{Smoothing noisy data with spline functions: estimating the correct degree of smoothing by the method of generalized cross-validation}, Numerische Mathematik, 31, 377–403, (1979)

\bibitem{art:marra}
Giampiero Marra, David L. Miller, Luca Zanin, \emph{Modelling the spatiotemporal distribution of the incidence of resident foreign population}, Statistica Neerlandica, 66, 133–160, (2012)

\bibitem{art:ramsay}
Timothy O. Ramsay, \emph{Spline smoothing over difficult regions}, Journal of the Royal Statistical Society: Series B, 64, 307–319, (2002)

\bibitem{art:sangalli}
Laura M. Sangalli, James O. Ramsay, Timothy O. Ramsay, \emph{Spatial spline regression models}, Journal of the Royal Statistical Society: Series B, 75, 681–703, (2013)

\bibitem{art:wood}
Simon N. Wood, Mark W. Bravington, Sharon L. Hedley, \emph{Soap film smoothing}, Journal of the Royal Statistical Society: Series B, 70, 931–955, (2008)


%\bibitem{prog:R}
%R Core Team, \emph{R: A Language and Environment for Statistical Computing}, R Foundation for Statistical Computing, Vienna, 2013, \url{http://www.R-project.org/}

\end{thebibliography}

\end{document}



%\[ 
%\underset{\scriptscriptstyle (m\times 1)}{Y}= 
%\underset{\scriptscriptstyle (m\times n)}{X} 
%\underset{\scriptscriptstyle (n\times 1)}{B} 
%\]
