\documentclass[a4paper,12pt]{report}				% COMANDI INIZIALI
\usepackage[italian]{babel}							% sillabazione italiana
\usepackage[utf8]{inputenc}							% Per le lettere accentate IN UNIX E IN WINDOWS
\usepackage{ragged2e}					 			% giustifica
\usepackage{amsmath}								% Per allineare le equazioni
\usepackage{amssymb}								% Per le lettere dell'indicatrice (mathbb)
\usepackage{bm}										% Per le lettere matematiche in grassetto (vettori)

\justifying 										% giustifica

\date{28 Luglio 2014}
\author{Gabriele Mazza}
\title{Dimostrazione caso con covariate}

\begin{document}

%Indice e numerazione
\pagenumbering{arabic}

\chapter{Caso con Covariate}

\section{Dimostrazione}
La funzione da minimizzare in questo caso sarà
$$
(\underline{z}-\bm{W}
$$
Per la dimostrazione di $S_{time}$ risulta tutto analogo alla dimostrazione del libro di Ramsay e Silverman.
\newline
Si ha come termine di penalizzazione da cui si ricava $S_{time}$:
$$
\sum_{i=1}^n J_T(a_i)=\sum_{i=1}^n \int_0^T (\frac{\partial^2a_i}{\partial t^2})^2 dt
$$
ma ogni coefficiente $a_i$ è tempo-variante secondo l'espansione
$$
a_i(t)=\sum_{j=1}^m c_{ij}\psi_j(t)
$$
la cui derivata seconda sarà
$$
\frac{\partial^2a_i}{\partial t^2}=\sum_{j=1}^m c_{ij}\psi''_j(t)= \underline{c_i}^t\underline{\psi}''_j(t) 
$$
Dove $\underline{c_i}$ è un vettore $m$-dimensionale contenente i coefficienti $c_{ij}$ corrispondenti al valore di $i$ che si sta considerando.
\newline
Allora si ha che, fissata $i$:
$$
\int_0^T (\frac{\partial^2a_i}{\partial t^2})^2 dt = \underline{c_i}^t S_{time} \underline{c_i}
$$
con
$$
S_{time,j1,j2}=\int_0^T\psi''_{j1}(t)\psi''_{j2}(t)dt
$$
Infatti, se sviluppassi il quadrato della derivata seconda della funzione integranda otterrei i quadrati
di ogni singolo termine e tutte le possibili combinazioni di doppi prodotti tra termini di indici differenti. Considerando a parte i corrispondenti coefficienti di $\underline{c}_j$, l'integrale è perfettamente ricomposto, termine per termine, dal prodotto matriciale $\underline{c_i}^t S_{time} \underline{c_i}$.
\newline
\newline
Quindi il problema successivo sarà sommare sui punti spaziali, e quindi rispetto ad $i$. Ma grazie al fatto che ho portato i coefficienti all'esterno della matrice $S_{time}$, è sufficiente usare il vettore $\underline{c}$ totalmente, si ha che
$$
\sum_{i=1}^n J_T(a_i)=\sum_{i=1}^n \int_0^T (\frac{\partial^2a_i}{\partial t^2})^2 dt=\underline{c}^t (S_{time}\otimes I_n) \underline{c}
$$
Quindi $(S_{time}\otimes I_n)$ è una matrice sparsa, in quanto è formata da tante sottomatrici diagonali.

\section{$S_{space}$}
Più complesso è trovare una formulazione per $S_{space}$. Credo che il problema sia legato al fatto che le funzioni $b_j$ che devo integrale sono già discrete.
\newline
Devo infatti semplificare il seguente integrale, per un fissato j
$$
\int_\Omega (\bigtriangleup b_j)^2d\Omega
$$
Che a differenza della relazione (17) dell'articolo \textit{Spatial spline regression model} non è però inserita in una equazione a questo punto del problema.
\newline
Tuttavia pongo $g_j=\bigtriangleup b_j$, e si ha:
$$
\int_\Omega g_j(\bigtriangleup b_j)d\Omega
$$
e sfrutto allora l'identità
$$
\int_\Omega g_jvd\Omega - \int_\Omega (\bigtriangleup b_j)vd\Omega=0
$$
Per ogni $v(p)=\sum_{i=0}^nv_i\varphi_i(p)$, funzione discretizzata in elementi finiti. Se le basi sono corrispondenti ai punti con i dati, sulla frontiera di $\Omega$ tutte le funzioni di base sono nulle e quindi anche ogni funzione $v$ lo è. Quindi se applico Green
$$
\int_\Omega (\bigtriangleup b_j)vd\Omega=-\int_\Omega (\nabla b_j)(\nabla v)d\Omega + \int_{\partial \Omega}v(\partial_{\nu}b_j)d\sigma
$$
il termine di integrale sul bordo di $\Omega$ è nullo.
\newline
$$
\int_{\Omega} g_jvd\Omega = -\int_{\Omega} (\nabla b_j)(\nabla v)d\Omega
$$
Essendo sia $b_j$ che $v$ funzioni già discretizzate, posso dire che il termine a destra coinvolge la matrice $\mathbf{R}_1$
$$
\int_{\Omega} g_jvd\Omega = -\underline{v}^t\mathbf{R}_1 \underline{c_j}
$$
Dove, analogamente a quanto fatto nella dimostrazione di $S_{time}$, $\underline{c_j}$ è il vettore con i coefficienti corrispondenti alla funzione $b_j$. 
\newline
A questo punto il problema: devo poter ridurre anche il primo integrale ad uno sviluppo matriciale. Di conseguenza devo introdurre una discretizzazione anche per le funzioni $g_j$, che però rappresentano il laplaciano delle $b_j$. Se facessi ciò, allora avrei:
$$
\underline{v}^t\mathbf{R}_0 \underline{g_j} = -\underline{v}^t\mathbf{R}_1 \underline{c_j}
$$
Dalla quale ricaverei, visto che la relazione è valida $\forall v$
$$
\underline{g_j}=-\mathbf{R}_0^-1\mathbf{R}_1\underline{c_j}
$$
E tornando poi all'integrale di partenza da semplificare, applico di nuovo Green:
$$
\int_\Omega g_j(\bigtriangleup b_j)d\Omega=-\int_{\Omega} (\nabla b_j)(\nabla g_j)d\Omega + \int_{\partial \Omega}g_j(\partial_{\nu}b_j)d\sigma
$$
Visto che nuovamente le funzioni $g_j$ avranno valore nullo sul bordo, in quanto discretizzate, elimino l'integrale sulla frontiera e si ha:
$$
-\int_{\Omega} (\nabla b_j)(\nabla g_j)d\Omega=-\underline{c_j}^t\mathbf{R}_1\underline{g_j}=\underline{c_j}^t\mathbf{R}_1\mathbf{R}_0^-1\mathbf{R}_1\underline{c_j}
$$
E quindi la scomposizione cercata. Ma ci sono alcuni punti deboli:
\begin{itemize}
\item è corretto supporre una discretizzazione anche per $g_j$ e quindi per il laplaciano delle funzioni $b_j$? Mi sembra un pò forzato, poichè non posso dire con certezza che il laplaciano di una funzione ad elementi finiti ha un espansione nella stessa base di elementi finiti.
\item se si sceglie un metodo diverso da quello che ho iniziato in questa dimostrazione, in ogni caso si cercherà di applicare la formula dell'integrazione per parti per semplificare il laplaciano delle $b_j$. Quindi si avrà sempre un integrale sul bordo del dominio contenente $\partial_{\nu}b_j$ che non posso porre automaticamente nullo poichè la funzione è già stata discretizzata con gli elementi. Potrei porlo vero più ad alto livello, se studiassi queste funzioni a livello infinito-dimensionale.
\end{itemize}
Quindi ho un problema legato a queste due ipotesi. Vorrei valutare se a questo punto è necessario eseguire questa dimostrazione più ad alto livello, cioè partendo dal caso infinito-dimensionale.
\end{document}